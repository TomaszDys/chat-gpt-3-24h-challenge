{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "word_embeddings = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True, limit=400)\n",
    "max_length = 329\n",
    "text = \"In sociology, dynamic density refers to the combination of two things: population density and the amount of social interaction within that population. Émile Durkheim used the term to explain why societies transition from simple to more complex forms, specifically in terms of the division of labor within that society. He suggested that it required both an increase in population and an increase in the frequency of social interaction to form more specialised occupations, which then leads to a new type of society. People in this new type of society are less independent and more reliant on each other and therefore develop what he called organic solidarity, where people no longer are bound by the same morality and sense of purpose. Critics suggest that it is not a testable hypothesis, and nor does it follow logically that dynamic density would cause this new type of solidarity, supposing it actually existed. ==Context== Dynamic density is a key component in Emile Durkheim’s theory of modernization. In his book The Division of Labor in Society ([1893] 1949), Durkheim suggests that over time, societies go through a transition from being more primitive, i.e. mechanical, to being more modern, or organic; the difference lying in the source of their solidarity, or what holds them together. (Ritzer, 2007) According to Durkheim, the cause of this transition is an increase in dynamic density, an idea he drew from earlier sociologists. \\\"Already Adam Smith had pointed to sufficient demand as a necessary condition for specialization, and Durkheim himself refers to Comte for the idea that density of interaction is the decisive factor [for transition to occur].\\\" (Rueschemeyer, 1982:580) Durkheim also borrows from Darwinian theory, and specifically from Darwin's The Origin of Species, for his explanation.(Rueschemeyer, 1982) In the animal kingdom, a single species of animal, like sheep, cannot survive in very high volumes on a given stretch of land because each animal makes exactly the same demands on that land. (Gibbs, 2003) They need to exist in symbiosis with other species, like the bees that fertilize the plants they consume, in order to thrive in greater numbers. (Gibbs, 2003) The same holds true within a human population. Had primitive societies increased in population density for many generations without an eventual specialization of tasks, competition for resources among the increasing number of people would have become so fierce that humans would have started dying off. (Merton, 1994) However, a growing population alone is not sufficient to spark a change in the division of labor, because individuals and small groups of people can live in relative isolation from one another and still perform most of the tasks necessary for survival themselves, no matter how big the overall population gets. (Ritzer, 2007) A growing population must also increase the frequency with which people interact within and between social groups; this increase in dynamic density is likely to spark a division of labor and the transformation of social solidarity. There are two types of social solidarity. The first is mechanical solidarity, where people are held together because they all serve the same purpose, or do the same things (as in a hunter-gatherer society), and their collective consciousness is therefore very strong. The people are all self-reliant, but they share the same experiences, understandings, and core beliefs, and can relate in that way. The second type of solidarity, organic solidarity, is the result of a substantial division of labor that has occurred due to much growth of dynamic density. People in organic solidarity have more specialized skills, so individuals are no longer self-sustaining. An example of this is that a philosopher has neither the time nor the ability to grow his own food, and so he is dependent upon a farmer and various other individuals so he can eat. In this situation, solidarity in society comes from the fact that people need the contribution of an increasing number of other people in order to function, and even to survive. (Ritzer, 2007) The transition from one type of solidarity to another is readily apparent in history when looking at societal changes from repressive law systems to restitutive law systems. A repressive law system is one in which any law breaker is severely punished for their crimes. This type of law exists in mechanical solidarity because the laws are based on the very powerful collective conscience, or set of social norms, that the people in a mechanical society all strongly believe in. Any violation of these beliefs is seen as an extreme offense against society as a whole. In contrast, a restitutive law system is characteristic of organic solidarity. Restitutive laws require an offender to pay for the harm he did to whoever was affected by his crimes, or he is just asked to comply with the law. With the growth of dynamic density and resulting division of labor in society, collective consciousness is weakened severely and people no longer have a unified sense of morality. (Muller, 1994) Everyone is no longer affected by or connected to every deviant action that takes place in society in organic solidarity, so the call for severity no longer exists. ==Critiques== There are some who disagree with Durkheim’s theory that dynamic density is the cause of social transition. Robert K. Merton argues that Durkheim has no empirical evidence supporting a link between dynamic density and a change from mechanical to organic solidarity. He says that Durkheim seeks to ignore the role that social driven ends themselves play into how society interacts. (Merton, 1994) Jack Gibbs also says that Durkheim’s theory of dynamic density leading to the division of labor is not scientifically testable for or evident of causality, arguing that there is no feasible way to measure the frequency of interactions between people, and thus no way to track progress or growth of said frequency; without these measurements, it is impossible to prove any correlation to division of labor. (Gibbs, 2003) Dietrich Rueschemeyer argues from an economics perspective that competition in production, which is the basis for a free market system, does not have the same consequences as Darwinian competition. (Rueschemeyer, 1982) To him, it follows logically that increased demand due to increased population density, for a product such as corn in an agrarian society, would improve rather than diminish the producers' chances of survival. (Rueschemeyer, 1982) Hence, it does not follow logically that dynamic density would cause the transition from mechanical to organic solidarity. ==See also== * Interaction frequency ==References== * Ritzer, George (2007)\\\"Contemporary Sociological Theory and Its Classical Roots; The Basics\\\" McGraw Hill * Merton, Robert K. (1994) \\\"Durkheim's Division of Labor in Society\\\" Plenum Press, NY and London Sociological Forum, Vol. 9, No. 1 * Muller, Hans-Peter (1994) \\\"Social DIfferentiation and Organic Solidarity: The Division of Labor Revisited\\\" Plenum Press, NY and London Sociological Forum, Vol. 9, No. 1 * Gibbs, Jack P. (2003) \\\"A Formal Restatement of Durkheim’s’ Division of Labor’ Theory\\\", Sociological Theory, Vol. 21, No. 2 * Rueschemeyer, Dietrich (1982)\\\"On Durkheim's Explanation of the Division of Labor\\\" The American Journal of Sociology, Vol. 88, No. 3 ==External links== * “Emile Durkheim on the Division of Labor” Category:Sociological terminology \"\n",
    "\n",
    "# add NER to word embedings\n",
    "# import spacy\n",
    "# import numpy as np\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# def create_custom_vector(ent, word_embeddings, ent_doc):\n",
    "#     ent_vector = None\n",
    "#     for token in ent_doc:\n",
    "#         if token.has_vector:\n",
    "#             ent_vector = token.vector\n",
    "#             ent_vector= np.pad(ent_vector, (0, 300 - len(ent_vector)), mode='constant')\n",
    "#             return ent_vector\n",
    "    \n",
    "# sentences= text.split('.')\n",
    "# max_length = max(map(len, sentences))\n",
    "# # Process the text\n",
    "# doc = nlp(text)\n",
    "\n",
    "# # Extract named entities\n",
    "# entities = [ent.text for ent in doc.ents]\n",
    "# # Create custom word vectors for named entities using pre-trained word embeddings\n",
    "# custom_embeddings = {}\n",
    "# for ent in entities:\n",
    "#     # check if the entity already exists in the pre-trained word embeddings\n",
    "#     if ent not in word_embeddings:\n",
    "#         # create a new word vector for the entity using the pre-trained embeddings\n",
    "#         word_embeddings[ent] = create_custom_vector(ent, word_embeddings, doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s>\n"
     ]
    }
   ],
   "source": [
    "print(word_embeddings.index_to_key[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(2, 329, 300), dtype=tf.float32, name=None), name='multi_head_attention/attention_output/add:0', description=\"created by layer 'multi_head_attention'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(2, 329, 329), dtype=tf.float32, name=None), name='model/layer_normalization_1/add:0', description=\"created by layer 'model'\")\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(2, 329)]                0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (2, 329, 300)             120000    \n",
      "                                                                 \n",
      " model (Functional)          (2, 329, 329)             105399    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,399\n",
      "Trainable params: 105,399\n",
      "Non-trainable params: 120,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, MultiHeadAttention, LayerNormalization, Dense, Flatten, Activation\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "from keras.layers import Embedding\n",
    "import numpy as np\n",
    "\n",
    "# Define the number of heads and the hidden size\n",
    "num_heads = 4\n",
    "hidden_size = 32\n",
    "\n",
    "# Create the embedding layer with the pre-trained embeddings\n",
    "embedding_layer = Embedding(input_dim=len(word_embeddings.index_to_key), output_dim=word_embeddings.vector_size, weights=[word_embeddings.vectors], trainable=False)\n",
    "\n",
    "# Define the input of the encoder block\n",
    "encoder_inputs = Input(shape=(max_length,),batch_size=2)\n",
    "# Pass the input through the embedding layer\n",
    "embedded_inputs = embedding_layer(encoder_inputs)\n",
    "# # Add the multi-head self-attention layer\n",
    "attention = MultiHeadAttention(num_heads, hidden_size)\n",
    "\n",
    "attention_output = attention(query = embedded_inputs, key = embedded_inputs, value = embedded_inputs)\n",
    "print(attention_output)\n",
    "\n",
    "# # Add the normalization layer\n",
    "normalized_attention = LayerNormalization()(attention_output)\n",
    "\n",
    "# # Add the feed-forward layer\n",
    "feed_forward = Dense(max_length, activation='relu')(normalized_attention)\n",
    "\n",
    "# Add the normalization layer\n",
    "normalized_feed_forward = LayerNormalization()(feed_forward)\n",
    "\n",
    "# Create the encoder block model\n",
    "encoder_block = Model(embedded_inputs, normalized_feed_forward)\n",
    "\n",
    "# Connect the output of the embedding layer to the input of the encoder block\n",
    "encoder_outputs = encoder_block(embedded_inputs)\n",
    "print(encoder_outputs)\n",
    "# Create the encoder model\n",
    "encoder_model = Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "\n",
    "\n",
    "def encoder_loss(y_true, y_pred):\n",
    "    # Define the loss function, in this case mean squared error\n",
    "    return tf.keras.metrics.mean_squared_error(y_true, y_pred)\n",
    "\n",
    "# Compile the model with a loss function and an optimizer\n",
    "encoder_model.compile(loss=encoder_loss, optimizer='adam')\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(2, 329)]                0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (2, 329, 300)             120000    \n",
      "                                                                 \n",
      " model (Functional)          (2, 329, 329)             105399    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 225,399\n",
      "Trainable params: 105,399\n",
      "Non-trainable params: 120,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Reshape,Attention,Dropout\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "import tensorflow as tf\n",
    "# Define the number of heads and the hidden size\n",
    "num_heads = 4\n",
    "hidden_size = 32\n",
    "\n",
    "# Define the input of the decoder\n",
    "decoder_inputs = encoder_outputs\n",
    "\n",
    "# Add the multi-head self-attention layer\n",
    "attention = MultiHeadAttention(num_heads, hidden_size)\n",
    "attention_output = attention(query=decoder_inputs, key=decoder_inputs, value=decoder_inputs)\n",
    "# Add the normalization layer\n",
    "normalized_attention = LayerNormalization()(attention_output)\n",
    "\n",
    "# Add the feed-forward layer\n",
    "feed_forward = Dense(hidden_size, activation='relu')(normalized_attention)\n",
    "\n",
    "# Add the normalization layer\n",
    "normalized_feed_forward = LayerNormalization()(feed_forward)\n",
    "# Create the decoder block model\n",
    "decoder_block = Model(decoder_inputs, normalized_feed_forward)\n",
    "# Apply the decoder block to the encoded output\n",
    "decoded_outputs = decoder_block(encoder_outputs)\n",
    "\n",
    "# Add a final Dense layer with a softmax activation\n",
    "final_dense = Dense(len(word_embeddings), activation='softmax')\n",
    "final_outputs = final_dense(decoded_outputs)\n",
    "# Create the decoder model\n",
    "decoder_model = Model(decoder_inputs, final_outputs)\n",
    "\n",
    "\n",
    "def decoder_loss(y_true, y_pred):\n",
    "    # Calculate the cross-entropy loss between the true target sequence and the predicted logits\n",
    "    loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_true, logits=y_pred)\n",
    "    # Mask out the loss for the padded tokens\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss = loss * mask\n",
    "    # Calculate the mean loss\n",
    "    loss = tf.reduce_mean(loss)\n",
    "    return loss\n",
    "\n",
    "decoder_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.01), loss=decoder_loss)\n",
    "encoder_model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dysko\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\numpy\\core\\fromnumeric.py:43: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  result = getattr(asarray(obj), method)(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 299ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 2s 2s/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4526e-18\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 9.9617e-09\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.2826\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0385\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0853\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 0.0664\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0401\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0138\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 0.0053\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 0.0028\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 8.3373e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 0.0013\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.7057e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.5590e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 8.5905e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 7.3479e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.3202e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.3697e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 5.5290e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.4822e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0168e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7238e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6891e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.8486e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.1516e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6646e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1187e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3325e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2637e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.9090e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 7.9683e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.3237e-05\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.8778e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 5.3018e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.7277e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 7.5002e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.7428e-05\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 2.6722e-17\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 3.7364e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0034e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5961e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.6940e-05\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3168e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.3365e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6057e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7581e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7730e-05\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.0343e-17\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5137e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.2219e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.3180e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3708e-05\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4803e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.4948e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.0071e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.6771e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 8.8806e-06\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.7887e-06\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.7991e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0540e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0845e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.1822e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4254e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5789e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3045e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.0159e-05\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 3.0659e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5239e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.6015e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.6266e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.4076e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8276e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4847e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5876e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.2772e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3124e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.7900e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.1371e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9752e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.1044e-05\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.0639e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.0785e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 8.8515e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0724e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 7.1065e-05\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1.0150e-19\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 32ms/step - loss: 7.8865e-06\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 1.0833e-05\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.2627e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.4299e-06\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.9612e-06\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.4895e-06\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0882e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.2358e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3534e-05\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.8412e-05\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.3868e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.8887e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.0690e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.6819e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8170e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 6.7874e-05\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3659e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.3258e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 39ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 5.0761e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 9.8422e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.7623e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.8894e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.0466e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 6.4226e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 4.7955e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.4119e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.5779e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 3.2684e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 7.3387e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.2804e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 4.6441e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.4077e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.7432e-04\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 7.7507e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.2534e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.5579e-04\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0193e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.4846e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.2522e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.3097e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.9334e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.4350e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 7.2943e-05\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 2.1130e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.6929e-05\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1171e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 3.5035e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.6982e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.8673e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.6281e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 3.1071e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.0956e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4463e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.8150e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.5648e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7455e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 4.1350e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.6284e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6779e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.6237e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.9840e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.8520e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 1.6048e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5961e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 8.3709e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.7928e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7156e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.2758e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5157e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 5.9525e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.7015e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.7905e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.4213e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4794e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 21ms/step - loss: 5.6724e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.6022e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0404e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3946e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 5.6718e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.5038e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 1.5972e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.3362e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3642e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 5.6208e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4913e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.1550e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.8198e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4397e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 4.2055e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.3599e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.7855e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0221e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4848e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.3508e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.2542e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 34ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.9417e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.4075e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 4.8176e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4376e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 2.1233e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.1697e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.4422e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7984e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.3883e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 4.6352e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.9541e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4133e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 4.9140e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.4523e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 2.1313e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 1.1780e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.4504e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.1511e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 35ms/step - loss: 1.3957e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 33ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 3.0094e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.5184e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 4.5307e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 2.5251e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8922e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.0820e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.5720e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 8.4476e-07\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.3504e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 38ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 3.2464e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 1.7382e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 5.8996e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 2.8435e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 4.4044e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.1576e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.7213e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 9.6071e-08\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4187e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.5830e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.0906e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.0467e-04\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 3.6088e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 9.9036e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7088e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 2.1084e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 9.7624e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 30ms/step - loss: 1.5485e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 31ms/step - loss: 0.0000e+00\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.7153e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.7549e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 6.0514e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 3.0525e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 22ms/step - loss: 2.6446e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.4840e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 1.8107e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.7218e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7673e-04\n",
      "2\n",
      "(2, 329)\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Epoch 1/10\n",
      "1/1 [==============================] - 0s 29ms/step - loss: 2.6722e-18\n",
      "Epoch 2/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 3.7751e-04\n",
      "Epoch 3/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 1.7444e-04\n",
      "Epoch 4/10\n",
      "1/1 [==============================] - 0s 23ms/step - loss: 6.5148e-05\n",
      "Epoch 5/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 3.0935e-04\n",
      "Epoch 6/10\n",
      "1/1 [==============================] - 0s 24ms/step - loss: 2.5623e-05\n",
      "Epoch 7/10\n",
      "1/1 [==============================] - 0s 25ms/step - loss: 1.5922e-04\n",
      "Epoch 8/10\n",
      "1/1 [==============================] - 0s 28ms/step - loss: 1.8281e-04\n",
      "Epoch 9/10\n",
      "1/1 [==============================] - 0s 26ms/step - loss: 8.9166e-06\n",
      "Epoch 10/10\n",
      "1/1 [==============================] - 0s 27ms/step - loss: 1.8834e-04\n"
     ]
    }
   ],
   "source": [
    "# Define the text and tokenize it into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "sentences = text.split('.')\n",
    "wordSentences = []\n",
    "for sentence in sentences:\n",
    "    s = word_tokenize(sentence)\n",
    "    sIndex = [word_embeddings.index_to_key.index(word) for word in s if word in word_embeddings]\n",
    "    wordSentences.append(sIndex)\n",
    "\n",
    "# Calculate the number of elements in the last array\n",
    "remainder = len(wordSentences) % 2\n",
    "\n",
    "# Duplicate the last element if necessary\n",
    "if remainder != 0:\n",
    "    wordSentences = np.append(wordSentences, wordSentences[-1])\n",
    "    wordSentences = np.append(wordSentences, wordSentences[-1])\n",
    "\n",
    "batches = np.array_split(wordSentences, (len(wordSentences)+1) / 2)\n",
    "\n",
    "for batch in batches:\n",
    "    # Pad the sequences to the same max_length\n",
    "    x_train_indices = batch\n",
    "    print(len(batch))\n",
    "    if(len(batch) != max_length):\n",
    "        if not(isinstance(batch[0], list) or isinstance(batch[0], np.ndarray)):\n",
    "            batch[0] = [batch[0]]\n",
    "        if not(isinstance(batch[1], list) or isinstance(batch[1], np.ndarray)):\n",
    "            batch[1] = [batch[1]]\n",
    "        x_train_indices = tf.keras.utils.pad_sequences(batch, maxlen=max_length, padding='post', value=0)\n",
    "    print(x_train_indices.shape)\n",
    "    y_train = encoder_model.predict(x_train_indices)\n",
    "\n",
    "    # Train the model with x_train and y_train\n",
    "    encoder_model.fit(x_train_indices, y_train, epochs=10, batch_size=2)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    # Pad the sequences to the same max_length\n",
    "    x_train_indices = batch\n",
    "    if(len(batch) != max_length):\n",
    "        if not(isinstance(batch[0], list) or isinstance(batch[0], np.ndarray)):\n",
    "            batch[0] = [batch[0]]\n",
    "        if not(isinstance(batch[1], list) or isinstance(batch[1], np.ndarray)):\n",
    "            batch[1] = [batch[1]]\n",
    "    x_train_indices = tf.keras.utils.pad_sequences(batch, maxlen=max_length, padding='post', value=0)\n",
    "    # Get the encoder output\n",
    "    y_train = [np.append(indexes[1:], 0) for indexes in x_train_indices]\n",
    "    # Train the decoder\n",
    "    encoder_prediction = encoder_model.predict(x_train_indices)\n",
    "    decoder_model.fit(encoder_prediction, tf.convert_to_tensor(y_train), epochs=15, batch_size=2, verbose=0)\n",
    "    \n",
    "    # My desprate tries to split the data in a way that model wont always return \"the\" :D\n",
    "    for time in range(len(batch[0])-2):\n",
    "        temp_batch = np.copy(batch)\n",
    "        temp_batch[0] = temp_batch[0][time:] \n",
    "        temp_batch[1] = temp_batch[1][time:] \n",
    "        x_train_indices = tf.keras.utils.pad_sequences(temp_batch, maxlen=max_length, padding='post', value=0)\n",
    "        y_train = [np.append(indexes[1:], 0) for indexes in x_train_indices]\n",
    "        encoder_prediction = encoder_model.predict(x_train_indices)\n",
    "        decoder_model.fit(encoder_prediction, tf.convert_to_tensor(y_train), epochs=15, batch_size=2, verbose=0)\n",
    "\n",
    "decoder_model.save('decoder.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 272ms/step\n"
     ]
    }
   ],
   "source": [
    "def pre_process(text):\n",
    "    sentences = text.split('.')\n",
    "    wordSentences = []\n",
    "    for sentence in sentences:\n",
    "        s = word_tokenize(sentence)\n",
    "        sIndex = [word_embeddings.index_to_key.index(word) for word in s if word in word_embeddings]\n",
    "        wordSentences.append(sIndex)\n",
    "\n",
    "    # Calculate the number of elements in the last array\n",
    "    remainder = len(wordSentences) % 2\n",
    "\n",
    "    # Duplicate the last element if necessary\n",
    "    if remainder == 1:\n",
    "        wordSentences = np.append(wordSentences, wordSentences[-1])\n",
    "\n",
    "    batches = np.array_split(wordSentences, len(wordSentences) / 2)\n",
    "\n",
    "    for batch in batches:\n",
    "        # Pad the sequences to the same max_length\n",
    "        x_train_indices = batch\n",
    "        if(len(batch) != max_length):\n",
    "            # those 2 ifs are just because splitting text to batches was not always creating arrays \n",
    "            if not(isinstance(batch[0], list) or isinstance(batch[0], np.ndarray)):\n",
    "                batch[0] = [batch[0]]\n",
    "            if not(isinstance(batch[1], list) or isinstance(batch[1], np.ndarray)):\n",
    "                batch[1] = [batch[1]]\n",
    "            x_train_indices = tf.keras.utils.pad_sequences(batch, maxlen=max_length, padding='post', value=0)\n",
    "            return x_train_indices\n",
    "\n",
    "input = pre_process(\"It was a state highway.\") \n",
    "\n",
    "test_e_output = encoder_model.predict(input)\n",
    "\n",
    "output = decoder_model.predict(test_e_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n",
      "the\n"
     ]
    }
   ],
   "source": [
    "test= np.argmax(output[0])\n",
    "print(test)\n",
    "print(word_embeddings.index_to_key[test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (tags/v3.10.9:1dd9be6, Dec  6 2022, 20:01:21) [MSC v.1934 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "892e5b2f45857642020bea3b025be691e6f02c169bb97a917cf7720410483fe3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
